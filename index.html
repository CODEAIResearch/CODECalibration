<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title> Beyond Confidence: Rethinking Uncertainty Calibration in Deep Code Models</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        figure {
            display: block;
            margin: 20px auto;
            text-align: center;
        }
        figure img {
            max-width: 80%;
            height: auto;
            border: 1px solid #ccc;
            border-radius: 8px;
        }
        figcaption {
            margin-top: 8px;
            font-size: 0.9em;
            color: #555;
        }
        .section-caption {
            text-align: center;
            font-weight: bold;
            margin-top: 40px;
            font-size: 1.5rem;
        }
    </style>
</head>

<body>
    <header>
        <h1>class="title is-3">Beyond Confidence: Rethinking Uncertainty Calibration in Deep Code Models </h1>
    </header>
    <!-- Abstract Section -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                        Deep learning models have shown significant promise in coderelated tasks. While confidence calibration techniques have been
proposed to improve model reliability, their effectiveness in realworld tasks with strict accuracy requirements remains uncertain. In
this paper, we investigate two key questions: first, whether existing
measurement metrics, including confidence scores and uncertainty
metrics, are effective for guiding selective prediction; and second,
whether existing calibration techniques can improve these metrics to better identify cases where model predictions are likely to
succeed or fail </br>
                            Through a systematic study using code models for defect prediction and vulnerability detection, our findings reveal that traditional
calibration metrics often fail to prioritize high-confidence predictions, which are crucial for practical deployment. To address this,
we propose a novel metric based on variation consistency across
model predictions, leveraging insights from ensemble methods
while reducing computational overhead. Additionally, we introduce
a weighted calibration technique that emphasizes high-confidence
predictions by assigning greater weight to confident outputs while
reducing the influence of low-confidence samples. </br>

                        Our empirical results show that the proposed methods improve
the ability to distinguish between correct and incorrect predictions
upto 96.3%, enhancing selective prediction performance. These
insights provide a promising direction for increasing the reliability
and practical utility of deep learning-based code models.
    </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Reliability Plots Section -->
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="section-caption">Example of Reliability Plots</h1>

            <h2 class="title is-4">Before Scaling</h2>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_before_scale-ucefrac.png" alt="BALD Before Scale UCEFrac">
                <figcaption>UCE Fraction Before Scaling</figcaption>
            </figure>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_before_scale-uncertainty.png" alt="BALD Before Scale Uncertainty">
                <figcaption>Uncertainty Scores Before Scaling</figcaption>
            </figure>

            <h2 class="title is-4">After Scaling</h2>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_after_scale-ucefrac.png" alt="BALD After Scale UCEFrac">
                <figcaption>UCE Fractions After Scaling</figcaption>
            </figure>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_after_scale-uncertainty.png" alt="BALD After Scale Uncertainty">
                <figcaption>Uncertainty Scores After Scaling</figcaption>
            </figure>

            <h2 class="title is-4">Platt Scaling</h2>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_plat_scaled-ucefrac.png" alt="BALD Platt Scaled UCEFrac">
                <figcaption>UCE Fraction with Platt Scaling</figcaption>
            </figure>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_plat_scaled-uncertainty.png" alt="BALD Platt Scaled Uncertainty">
                <figcaption>Uncertainty Scores After Platt Scaling</figcaption>
            </figure>

            <h2 class="title is-4">Isotonic Regression</h2>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/baldisotonic-ucefrac.png" alt="BALD Isotonic UCEFrac">
                <figcaption>UCE Fraction After Isotonic Regression </figcaption>
            </figure>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/baldisotonic-uncertainty.png" alt="BALD Isotonic Uncertainty">
                <figcaption>Uncertainty Scores with Isotonic Regression</figcaption>
            </figure>
        </div>
    </section>
</body>
</html>
