<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title> Beyond Confidence: Rethinking Uncertainty Calibration in Deep Code Models</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        figure {
            display: block;
            margin: 20px auto;
            text-align: center;
        }
        figure img {
            max-width: 80%;
            height: auto;
            border: 1px solid #ccc;
            border-radius: 8px;
        }
        figcaption {
            margin-top: 8px;
            font-size: 0.9em;
            color: #555;
        }
        .section-caption {
            text-align: center;
            font-weight: bold;
            margin-top: 40px;
            font-size: 1.5rem;
        }
    </style>
</head>

<body>
    <header>
        <h1 class="title is-3">Beyond Confidence: Rethinking Uncertainty Calibration in Deep Code Models </h1>
    </header>
    <!-- Abstract Section -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                        Deep learning models have demonstrated strong performance in code-related tasks, yet their reliability remains
limited by poor uncertainty calibration. Existing techniques focus on aligning predicted confidence with
correctness in a numerical sense, but often fail to support reliable decision-making in practice. This paper
identifies a core limitation: numerical calibration alone is insufficient, especially in settings where models
must decide whether to answer or abstain. </br>
                            We conduct a systematic study to assess whether commonly used confidence and measurement metrics
are effective for guiding selective prediction in deep code models, and whether standard post-hoc calibration
techniques improve their ability to distinguish reliable from unreliable predictions. Across two tasks, we
evaluate 13 uncertainty metrics and three calibration methods over four state-of-the-art models, revealing
that conventional techniques often misrepresent prediction quality, particularly in high-confidence regions. </br>

                        To address this, we introduce a lightweight behavioral calibration method, a post-hoc method that estimates
per-instance correctness likelihoods from logit-derived features and applies a threshold-based rule for selective
prediction. Our method improves miscorrection validation rate by up to 68.8% and selective accuracy by up to
15.6%, across all models and tasks. These findings establish behavioral calibration as a necessary direction for
enhancing the reliability of deep code models beyond numeric calibration alone.
    </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

<details>
  <summary>MECE/MUCE scores</summary>
  Due to README space limits, see the full table here:
  <a href="https://github.com/CODEAIResearch/CODECalibration/blob/main/MECE_MUCE_SCORES/uncertainty_metrics.csv">uncertainty_metrics.csv</a>.
</details>


<section id="motivation">
  <h2>Motivation</h2>
  <p>
    Modern deep code models are increasingly deployed for vulnerability detection, defect prediction, and other
    high-impact software engineering tasks, yet their predictive confidence is systematically unreliable.
    Overconfident mispredictions on security- and quality-critical code fragments introduce concrete deployment risks,
    particularly when model scores are integrated into CI pipelines, automated triage, or semi-autonomous decision flows.
  </p>
  <p>
    Conventional calibration techniques and uncertainty metrics primarily target distribution-level summaries
    (e.g., deviations between predicted confidence and empirical accuracy), without ensuring that
    high-confidence outputs correspond to decisions that can be safely trusted.
    This study addresses the need to transform inherently unreliable code models into
    <strong>partially reliable decision components</strong> by:
    (i) rigorously evaluating whether existing uncertainty estimation and calibration methods
    support robust selective prediction and abstention, and
    (ii) introducing calibration strategies explicitly aligned with instance-level correctness
    rather than cosmetic improvements in aggregate metrics.
  </p>
</section>

    <section id="main-findings">
  <h2>Main Findings</h2>

  <h3>1. Strong miscalibration in deep code models</h3>
  <p>
    Deep code classifiers for vulnerability detection and defect prediction exhibit substantial gaps between
    predicted confidence and empirical accuracy, with severe overconfidence on erroneous predictions in
    decision-critical regions.
  </p>

  <h3>2. Sensitivity to task, model, and metric choice</h3>
  <p>
    No single uncertainty metric consistently separates correct from incorrect predictions across architectures,
    datasets, and tasks. Metrics that appear effective in one configuration can degrade reliability in another,
    indicating strong context dependence.
  </p>

  <h3>3. Limited effectiveness of standard post-hoc calibration</h3>
  <p>
    Temperature scaling, Platt scaling, and isotonic regression frequently reduce global calibration error
    but do not reliably improve selective prediction performance or high-confidence decision safety.
    Lower ECE/UCE does not guarantee safer acceptance of model outputs.
  </p>

  <h3>4. Misalignment between standard metrics and deployment needs</h3>
  <p>
    Aggregate calibration measures obscure behavior on high-confidence subsets, where even a small number
    of catastrophic failures is unacceptable. Models can satisfy global calibration criteria while remaining
    unreliable in precisely the operating regimes practitioners depend on.
  </p>

  <h3>5. Fragility of naive selective prediction</h3>
  <p>
    Thresholding raw confidence or generic uncertainty scores yields unstable selective predictors:
    either many incorrect predictions are classified as confident, or large fractions of correct
    predictions are unnecessarily rejected, limiting effective partial automation.
  </p>

  <h3>6. Behavioral calibration as a deployment-aligned objective</h3>
  <p>
    Reframing calibration as per-instance correctness estimation produces behaviors better matched to
    practical reliability requirements. Utilizing richer information from the full logit distribution,
    rather than only the top-class probability, enables more accurate discrimination between trustworthy
    and untrustworthy predictions.
  </p>

  <h3>7. Logit-based calibration improves partial reliability</h3>
  <p>
    A lightweight logit-based calibration approach improves selective prediction by increasing accuracy
    at fixed coverage, capturing a larger fraction of wrong predictions, and reducing unnecessary abstention
    on correct predictions. This enhancement is achieved without modifying or retraining the underlying models.
  </p>
</section>

<section id="key-insights">
  <h2>Key Insights and Takeaways</h2>

  <h3>1. Reliability is fundamentally instance-level</h3>
  <p>
    For practical code intelligence, the central question is whether a specific prediction is safe to act on.
    Calibration and evaluation must explicitly target and report instance-level correctness estimation.
  </p>

  <h3>2. Standard calibration metrics are not sufficient</h3>
  <p>
    ECE/UCE-style scores alone are inadequate indicators of deployment safety.
    Any claim of reliability must be supported by selective prediction behavior, coverageâ€“risk trade-offs,
    and error analysis in high-confidence regions.
  </p>

  <h3>3. Uncertainty is multi-dimensional and context-dependent</h3>
  <p>
    No single uncertainty measure is universally robust for code tasks.
    Reliable systems should treat uncertainty as a composite signal and validate it per model, per dataset,
    and per application scenario.
  </p>

  <h3>4. Selective prediction is the right abstraction for partial automation</h3>
  <p>
    Positioning code models as components that can abstain under uncertainty is more realistic than expecting
    uniformly accurate predictions. Effective abstention, however, requires behaviorally aligned calibration
    instead of naive confidence thresholding.
  </p>

  <h3>5. Logit-informed calibration is a practical upgrade path</h3>
  <p>
    Exploiting the structure of the logit distribution to estimate correctness provides a scalable,
    model-agnostic mechanism to convert existing deep code models into partially reliable predictors,
    suitable for integration into security and quality assurance workflows without retraining the base models.
  </p>
</section>
    <!-- Reliability Plots Section -->
    <!--
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="section-caption">Example of Reliability Plots</h1>

            <h2 class="title is-4">Before Scaling</h2>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_before_scale-ucefrac.png" alt="BALD Before Scale UCEFrac">
                <figcaption>UCE Fraction Before Scaling</figcaption>
            </figure>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_before_scale-uncertainty.png" alt="BALD Before Scale Uncertainty">
                <figcaption>Uncertainty Scores Before Scaling</figcaption>
            </figure>

            <h2 class="title is-4">After Scaling</h2>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_after_scale-ucefrac.png" alt="BALD After Scale UCEFrac">
                <figcaption>UCE Fractions After Scaling</figcaption>
            </figure>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_after_scale-uncertainty.png" alt="BALD After Scale Uncertainty">
                <figcaption>Uncertainty Scores After Scaling</figcaption>
            </figure>

            <h2 class="title is-4">Platt Scaling</h2>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_plat_scaled-ucefrac.png" alt="BALD Platt Scaled UCEFrac">
                <figcaption>UCE Fraction with Platt Scaling</figcaption>
            </figure>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_plat_scaled-uncertainty.png" alt="BALD Platt Scaled Uncertainty">
                <figcaption>Uncertainty Scores After Platt Scaling</figcaption>
            </figure>

            <h2 class="title is-4">Isotonic Regression</h2>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/baldisotonic-ucefrac.png" alt="BALD Isotonic UCEFrac">
                <figcaption>UCE Fraction After Isotonic Regression </figcaption>
            </figure>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/baldisotonic-uncertainty.png" alt="BALD Isotonic Uncertainty">
                <figcaption>Uncertainty Scores with Isotonic Regression</figcaption>
            </figure>
        </div>
    </section>
    -->
</body>
</html>
