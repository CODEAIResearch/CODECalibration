<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title> Beyond Confidence: Rethinking Uncertainty Calibration in Deep Code Models</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        figure {
            display: block;
            margin: 20px auto;
            text-align: center;
        }
        figure img {
            max-width: 80%;
            height: auto;
            border: 1px solid #ccc;
            border-radius: 8px;
        }
        figcaption {
            margin-top: 8px;
            font-size: 0.9em;
            color: #555;
        }
        .section-caption {
            text-align: center;
            font-weight: bold;
            margin-top: 40px;
            font-size: 1.5rem;
        }
    </style>
</head>

<body>
    <header>
        <h1 class="title is-3">Beyond Confidence: Rethinking Uncertainty Calibration in Deep Code Models </h1>
    </header>
    <!-- Abstract Section -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                        Deep learning models have demonstrated strong performance in code-related tasks, yet their reliability remains
limited by poor uncertainty calibration. Existing techniques focus on aligning predicted confidence with
correctness in a numerical sense, but often fail to support reliable decision-making in practice. This paper
identifies a core limitation: numerical calibration alone is insufficient, especially in settings where models
must decide whether to answer or abstain. </br>
                            We conduct a systematic study to assess whether commonly used confidence and measurement metrics
are effective for guiding selective prediction in deep code models, and whether standard post-hoc calibration
techniques improve their ability to distinguish reliable from unreliable predictions. Across two tasks, we
evaluate 13 uncertainty metrics and three calibration methods over four state-of-the-art models, revealing
that conventional techniques often misrepresent prediction quality, particularly in high-confidence regions. </br>

                        To address this, we introduce a lightweight behavioral calibration method, a post-hoc method that estimates
per-instance correctness likelihoods from logit-derived features and applies a threshold-based rule for selective
prediction. Our method improves miscorrection validation rate by up to 68.8% and selective accuracy by up to
15.6%, across all models and tasks. These findings establish behavioral calibration as a necessary direction for
enhancing the reliability of deep code models beyond numeric calibration alone.
    </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

<details>
  <summary>MECE/MUCE scores</summary>
  Due to README space limits, see the full table here:
  <a href="https://github.com/CODEAIResearch/CODECalibration/blob/main/MECE_MUCE_SCORES/uncertainty_metrics.csv">uncertainty_metrics.csv</a>.
</details>

    <!-- Reliability Plots Section -->
    <!--
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="section-caption">Example of Reliability Plots</h1>

            <h2 class="title is-4">Before Scaling</h2>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_before_scale-ucefrac.png" alt="BALD Before Scale UCEFrac">
                <figcaption>UCE Fraction Before Scaling</figcaption>
            </figure>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_before_scale-uncertainty.png" alt="BALD Before Scale Uncertainty">
                <figcaption>Uncertainty Scores Before Scaling</figcaption>
            </figure>

            <h2 class="title is-4">After Scaling</h2>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_after_scale-ucefrac.png" alt="BALD After Scale UCEFrac">
                <figcaption>UCE Fractions After Scaling</figcaption>
            </figure>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_after_scale-uncertainty.png" alt="BALD After Scale Uncertainty">
                <figcaption>Uncertainty Scores After Scaling</figcaption>
            </figure>

            <h2 class="title is-4">Platt Scaling</h2>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_plat_scaled-ucefrac.png" alt="BALD Platt Scaled UCEFrac">
                <figcaption>UCE Fraction with Platt Scaling</figcaption>
            </figure>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/bald_plat_scaled-uncertainty.png" alt="BALD Platt Scaled Uncertainty">
                <figcaption>Uncertainty Scores After Platt Scaling</figcaption>
            </figure>

            <h2 class="title is-4">Isotonic Regression</h2>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/baldisotonic-ucefrac.png" alt="BALD Isotonic UCEFrac">
                <figcaption>UCE Fraction After Isotonic Regression </figcaption>
            </figure>
            <figure>
                <img src="./images/BALD-DE-CodeBERT-VD/baldisotonic-uncertainty.png" alt="BALD Isotonic Uncertainty">
                <figcaption>Uncertainty Scores with Isotonic Regression</figcaption>
            </figure>
        </div>
    </section>
    -->
</body>
</html>
