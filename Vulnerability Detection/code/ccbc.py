from __future__ import absolute_import, division, print_function

import argparse
import logging
import os

import math

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset, SequentialSampler,RandomSampler
from torch.utils.data.distributed import DistributedSampler
import json


from tqdm import tqdm, trange
import multiprocessing
from model import Model, DecoderClassifier

cpu_cont = multiprocessing.cpu_count()

from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed

from transformers import BitsAndBytesConfig, TrainingArguments, pipeline
from peft import LoraConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.optim import AdamW
cpu_cont = multiprocessing.cpu_count()
from torch.optim import AdamW
from transformers import (WEIGHTS_NAME, get_linear_schedule_with_warmup,
                          BertConfig, BertForMaskedLM, BertTokenizer, BertForSequenceClassification,
                          GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,
                          OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,
                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,
                          DistilBertConfig, DistilBertForMaskedLM, DistilBertForSequenceClassification, DistilBertTokenizer)

logger = logging.getLogger(__name__)

from transformers import (WEIGHTS_NAME, get_linear_schedule_with_warmup, get_scheduler,
                          BertConfig, BertForMaskedLM, BertTokenizer, BertForSequenceClassification,
                          GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,
                          OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,
                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,
                          DistilBertConfig, DistilBertForMaskedLM, DistilBertForSequenceClassification, DistilBertTokenizer, 
                          AutoConfig, AutoModel, AutoTokenizer)

logger = logging.getLogger(__name__)

MODEL_CLASSES = {
    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),
    'openai-gpt': (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),
    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),
    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),
    'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer),
    'codellama' : (AutoConfig,AutoModel, AutoTokenizer),
    'deepseek' : (AutoConfig,AutoModel, AutoTokenizer),
    'qwen7b' : (AutoConfig,AutoModel, AutoTokenizer),
    'starcoder3b' : (AutoConfig,AutoModel, AutoTokenizer),
    'incoder1b' : (AutoConfig,AutoModel, AutoTokenizer),
    'codegemma' : (AutoConfig,AutoModel, AutoTokenizer)
    }


from plots import draw_uce_reliability_graph, draw_reliability_graph
from sklearn.metrics import roc_auc_score
#from emsemble import *
from scaling import *
from platt_scaling import *

from torch import nn, optim
from torch.nn import functional as F



class InputFeatures(object):
    """A single training/test features for a example."""
    def __init__(self,
                 input_tokens,
                 input_ids,
                 idx,
                 label,

    ):
        self.input_tokens = input_tokens
        self.input_ids = input_ids
        self.idx=str(idx)
        self.label=label

def convert_examples_to_features(js,tokenizer,args):
    code = js['func']
    if args.model_type in ["codellama"]:
        code_tokens = tokenizer.tokenize(code)
        if '</s>' in code_tokens:
            code_tokens = code_tokens[:code_tokens.index('</s>')]
        source_tokens = code_tokens[:args.block_size]
    elif args.model_type in ["starcoder3b", "deepseek", "incoder1b",'qwen7b']:
        code_tokens=tokenizer.tokenize(code)
        source_tokens = code_tokens[:args.block_size]
    else:
        code_tokens=tokenizer.tokenize(code)
        code_tokens = code_tokens[:args.block_size-2]
        source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]
    if args.model_type in ["codellama"]:
        source_ids = tokenizer.encode(js['func'].split("</s>")[0], max_length=args.block_size, padding='max_length', truncation=True)
    else:
        source_ids = tokenizer.encode(
            code,
            max_length=args.block_size,
            padding='max_length',
            truncation=True,
            add_special_tokens=False,  # decoder-only style
        )

        #source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)
        #padding_length = args.block_size - len(source_ids)
        #source_ids+=[tokenizer.pad_token_id]*padding_length
    return InputFeatures(source_tokens,source_ids,js['idx'],js['target'])

class TextDataset(Dataset):
    def __init__(self, tokenizer, args, file_path=None):
        self.examples = []
        with open(file_path) as f:
            for line in f:
                js=json.loads(line.strip())
                self.examples.append(convert_examples_to_features(js,tokenizer,args))


    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):       
        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)

import random
def set_seed(seed=42):
    random.seed(seed)
    os.environ['PYHTONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True






def get_predictions(model, dataloader, args):
        
        prediction_probs = []
        labels_list = []
        pred_list = []
        logit_list = []

        model.eval()

        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Evaluating"):
                inputs = batch[0].cuda()
                labels = batch[1].cuda()
                probs,logits = model.forward_with_logits(input_ids=inputs)

                 #F.softmax(logits, dim=-1)
               
                preds = torch.argmax(probs, dim=1)
                
                

                prediction_probs.append(probs)
                logit_list.append(logits)
                
                labels_list.append(labels)
                pred_list.append(preds) # predictions


        predictions = torch.cat(prediction_probs).cuda() 
        label = torch.cat(labels_list).cuda()
        pred = torch.cat(pred_list).cuda()
        logit = torch.cat(logit_list).cuda()

        return predictions, label, pred,logit



def compute_vanilla(probabilities):
    uncertainties, _= torch.max(probabilities,dim=1)

    return uncertainties


# behavioral_calibration_full.py
# Post-hoc behavioral calibration for code models:
# - Features from logits: Energy + Top-2 Logit Margin (+ optional SR)
# - Class-conditional or global "vector Platt" (logistic) or monotone (isotonic) calibration
# - Coverage-aware selection: exact (oracle) or deploy-like calibrated thresholds
# - Metrics: selective accuracy & risk by coverage, AURC, ECE, per-class coverage/accuracy
#
# Inputs expected from your pipeline:
#   eval_probs, eval_label, eval_preds, eval_logits = get_predictions(model, eval_dataloader, args)
#   test_probs, test_label, test_preds, test_logits = get_predictions(model, test_dataloader, args)

import numpy as np
import torch
import pandas as pd
from typing import Dict, List, Tuple, Literal, Optional
from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression


# ------------------------------ Feature extraction ------------------------------

@torch.no_grad()
def compute_energy_and_margin(logits: torch.Tensor) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Args:
        logits: [N, K] torch tensor (any device/dtype)
    Returns:
        energy:  [N] numpy, E(x) = -logsumexp(z)
        margin:  [N] numpy, Δz   = z_(1) - z_(2)
        yhat:    [N] numpy, predicted class (argmax logits)
    """
    z = logits.float()
    energy = -torch.logsumexp(z, dim=1)                  # [N]
    top2 = torch.topk(z, k=2, dim=1).values             # [N,2]
    margin = top2[:, 0] - top2[:, 1]                    # [N]
    yhat = torch.argmax(z, dim=1)                       # [N]
    return energy.cpu().numpy(), margin.cpu().numpy(), yhat.cpu().numpy()


def make_features(
    logits: torch.Tensor,
    probs: Optional[torch.Tensor] = None,
    use_logits_margin: bool = True,
    include_sr: bool = True
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Build a small feature vector from logits (and optionally probs).
    Features:
      - energy E(z)
      - logit margin Δz = z_(1) - z_(2)
      - SR = p_max (from probs if provided, else softmax(logits))
    Returns:
      X  : [N, D] numpy array
      ŷ  : [N] predicted class (from logits)
    """
    E, M, yhat = compute_energy_and_margin(logits)
    feats = [E]
    if use_logits_margin:
        feats.append(M)
    if include_sr:
        if probs is not None:
            pmax = probs.float().max(dim=1).values
        else:
            pmax = torch.softmax(logits.float(), dim=1).amax(dim=1)
        feats.append(pmax.cpu().numpy())
    X = np.stack(feats, axis=1)
    return X, yhat


# ------------------------------ Calibrators ------------------------------

class Calibrator:
    """
    Flexible correctness calibrator:
      mode ∈ { "global_logistic", "global_isotonic", "perclass_logistic", "perclass_isotonic" }
    - logistic: multi-D logistic regression → P(correct | X)
    - isotonic: learn a 1D projection by logistic, then monotone isotonic on that score
    """
    def __init__(self, mode: Literal["global_logistic", "global_isotonic",
                                     "perclass_logistic", "perclass_isotonic"] = "global_logistic"):
        self.mode = mode
        self.models = {}           # dict of models
        self.global_proj = None    # global logistic projection for isotonic fallback

    def fit(self, X: np.ndarray, yhat: np.ndarray, correct: np.ndarray, K: int, min_samples: int = 25):
        correct = correct.astype(int)

        # Prepare global projection (for isotonic fallback and/or global_isotonic)
        lr_global = LogisticRegression(max_iter=1000)
        lr_global.fit(X, correct)
        self.global_proj = lr_global

        if self.mode == "global_logistic":
            self.models["global"] = lr_global

        elif self.mode == "global_isotonic":
            s = lr_global.decision_function(X)  # 1D score
            iso = IsotonicRegression(out_of_bounds="clip")
            iso.fit(s, correct)
            self.models["global"] = iso

        elif self.mode == "perclass_logistic":
            for c in range(K):
                idx = (yhat == c)
                if idx.sum() >= min_samples:
                    lr = LogisticRegression(max_iter=1000)
                    lr.fit(X[idx], correct[idx])
                    self.models[c] = lr
                else:
                    self.models[c] = lr_global  # fallback

        elif self.mode == "perclass_isotonic":
            # per-class isotonic via per-class projection; fallback to global isotonic
            s_global = lr_global.decision_function(X)
            iso_global = IsotonicRegression(out_of_bounds="clip").fit(s_global, correct)

            for c in range(K):
                idx = (yhat == c)
                if idx.sum() >= min_samples:
                    lr = LogisticRegression(max_iter=1000)
                    lr.fit(X[idx], correct[idx])
                    s = lr.decision_function(X[idx])
                    iso = IsotonicRegression(out_of_bounds="clip")
                    iso.fit(s, correct[idx])
                    self.models[c] = (lr, iso)
                else:
                    self.models[c] = (lr_global, iso_global)

        else:
            raise ValueError(f"Unknown mode: {self.mode}")

    def predict(self, X: np.ndarray, yhat: np.ndarray, K: int) -> np.ndarray:
        if self.mode == "global_logistic":
            return self.models["global"].predict_proba(X)[:, 1]

        if self.mode == "global_isotonic":
            s = self.global_proj.decision_function(X)
            return self.models["global"].predict(s)

        if self.mode == "perclass_logistic":
            p = np.zeros(X.shape[0], dtype=float)
            for c in range(K):
                idx = (yhat == c)
                if not idx.any(): continue
                p[idx] = self.models[c].predict_proba(X[idx])[:, 1]
            return p

        if self.mode == "perclass_isotonic":
            p = np.zeros(X.shape[0], dtype=float)
            for c in range(K):
                idx = (yhat == c)
                if not idx.any(): continue
                lr, iso = self.models[c]
                s = lr.decision_function(X[idx])
                p[idx] = iso.predict(s)
            return p

        raise ValueError(f"Unknown mode: {self.mode}")


# ------------------------------ Selection (coverage control) ------------------------------

def thresholds_per_class_for_target_coverage(
    p_correct_eval: np.ndarray, yhat_eval: np.ndarray, K: int, target_cov: float
) -> Dict[int, float]:
    """
    Per-class quantile thresholds (fit on evaluation/calibration split).
    Achieves per-class coverage ≈ target on eval; test coverage will be close (deploy-like).
    """
    taus = {}
    q = 1.0 - float(target_cov)
    for c in range(K):
        idx = (yhat_eval == c)
        if idx.sum() == 0:
            taus[c] = 1.1  # accept none
        else:
            taus[c] = float(np.quantile(p_correct_eval[idx], q))
    return taus


def decide_accept_per_class(p_correct_test: np.ndarray, yhat_test: np.ndarray, taus: Dict[int, float]) -> np.ndarray:
    accept = np.zeros_like(p_correct_test, dtype=bool)
    for c, tau in taus.items():
        idx = (yhat_test == c)
        if idx.any():
            accept[idx] = (p_correct_test[idx] >= tau)
    return accept


def accept_with_exact_global_coverage(p_correct_test: np.ndarray, target_cov: float, deterministic: bool = True) -> np.ndarray:
    """
    Oracle selection: accept exactly top-K by p_correct on TEST so coverage == target_cov.
    This is for evaluation curves (not deploy thresholds). Ties handled deterministically.
    """
    N = len(p_correct_test)
    K = int(round(target_cov * N))
    if K <= 0:
        return np.zeros(N, dtype=bool)
    if K >= N:
        return np.ones(N, dtype=bool)

    order = np.argsort(p_correct_test)[::-1]  # descending
    cutoff = p_correct_test[order[K-1]]
    accept = (p_correct_test > cutoff)

    # Handle ties
    tie_idx = np.where(p_correct_test == cutoff)[0]
    need = K - accept.sum()
    if need > 0 and tie_idx.size > 0:
        chosen = np.sort(tie_idx)[:need] if deterministic else np.random.choice(tie_idx, size=need, replace=False)
        accept[chosen] = True
    return accept


# ------------------------------ Metrics ------------------------------

def selective_metrics(labels: np.ndarray, yhat: np.ndarray, accept: np.ndarray, K: int):
    """
    Returns:
      coverage, selective_accuracy (%), risk (%),
      per_class_coverage dict, per_class_selective_accuracy dict (%)
    """
    N = len(labels)
    accepted = int(accept.sum())
    coverage = accepted / max(N, 1)

    if accepted == 0:
        per_class_cov = {c: 0.0 for c in range(K)}
        per_class_acc = {c: 0.0 for c in range(K)}
        return coverage, 0.0, 100.0, per_class_cov, per_class_acc

    sel_acc = (yhat[accept] == labels[accept]).mean() * 100.0
    risk = 100.0 - sel_acc

    per_class_cov, per_class_acc = {}, {}
    for c in range(K):
        idx_c = (yhat == c)
        n_c = int(idx_c.sum())
        if n_c == 0:
            per_class_cov[c] = 0.0
            per_class_acc[c] = 0.0
            continue
        acc_idx = idx_c & accept
        per_class_cov[c] = acc_idx.sum() / n_c
        if acc_idx.sum() > 0:
            per_class_acc[c] = (yhat[acc_idx] == labels[acc_idx]).mean() * 100.0
        else:
            per_class_acc[c] = 0.0

    return float(coverage), float(sel_acc), float(risk), per_class_cov, per_class_acc


def ece_binary(p: np.ndarray, correct: np.ndarray, n_bins: int = 15) -> float:
    """
    ECE for correctness probability. Returns percent (%).
    """
    p = p.clip(1e-6, 1 - 1e-6)
    bins = np.linspace(0.0, 1.0, n_bins + 1)
    N = len(p)
    ece = 0.0
    for b in range(n_bins):
        lo, hi = bins[b], bins[b + 1]
        idx = (p >= lo) & (p < hi) if b < n_bins - 1 else (p >= lo) & (p <= hi)
        n = idx.sum()
        if n == 0:
            continue
        conf = p[idx].mean()
        acc = correct[idx].mean()
        ece += (n / N) * abs(conf - acc)
    return ece * 100.0


def aurc(coverages: List[float], risks: List[float]) -> float:
    """
    Area under risk–coverage curve (trapezoid). Risk is in %.
    """
    cov = np.array(coverages, dtype=float)
    rsk = np.array(risks, dtype=float)
    order = np.argsort(cov)
    cov = cov[order]; rsk = rsk[order]
    return float(np.trapz(rsk, cov))


# ------------------------------ Main pipeline ------------------------------

def behavioral_calibration_pipeline(
    # eval/cal split
    eval_logits: torch.Tensor, eval_preds: torch.Tensor, eval_labels: torch.Tensor,
    eval_probs: Optional[torch.Tensor],
    # test split
    test_logits: torch.Tensor, test_preds: torch.Tensor, test_labels: torch.Tensor,
    test_probs: Optional[torch.Tensor],
    num_classes: int,
    coverages: List[float],
    min_samples: int = 25,
    calibrator_mode: Literal["global_logistic", "global_isotonic", "perclass_logistic", "perclass_isotonic"] = "global_logistic",
    selection_mode: Literal["oracle_exact", "calibrated"] = "oracle_exact",
    csv_path: Optional[str] = None,
    use_logits_margin: bool = True,
    include_sr_feature: bool = True,
):
    """
    Runs class-conditional/global behavioral calibration and returns metrics.
    If csv_path is provided, writes a per-coverage CSV with thresholds and per-class stats.
    """
    K = int(num_classes)

    # Build features
    X_eval, yhat_eval = make_features(eval_logits, probs=eval_probs,
                                      use_logits_margin=use_logits_margin, include_sr=include_sr_feature)
    X_test, yhat_test = make_features(test_logits, probs=test_probs,
                                      use_logits_margin=use_logits_margin, include_sr=include_sr_feature)

    # Correctness labels (0/1)
    eval_correct = (eval_preds.detach().cpu().numpy() == eval_labels.detach().cpu().numpy()).astype(int)
    test_correct = (test_preds.detach().cpu().numpy() == test_labels.detach().cpu().numpy()).astype(int)

    # Fit calibrator
    cal = Calibrator(mode=calibrator_mode)
    cal.fit(X_eval, yhat_eval, eval_correct, K, min_samples=min_samples)

    # Predict p(correct)
    p_eval = cal.predict(X_eval, yhat_eval, K)
    p_test = cal.predict(X_test, yhat_test, K)

    # Diagnostics
    ece = ece_binary(p_test, test_correct, n_bins=15)
    baseline_acc = float((test_preds.detach().cpu().numpy() == test_labels.detach().cpu().numpy()).mean() * 100.0)

    # Sweep coverages
    rows = []
    covs, risks = [], []
    labels_np = test_labels.detach().cpu().numpy()
    yhat_np = test_preds.detach().cpu().numpy()

    for cov_target in coverages:
        if selection_mode == "oracle_exact":
            accept = accept_with_exact_global_coverage(p_correct_test=p_test, target_cov=cov_target, deterministic=True)
            tau_dump = {"global_test_quantile": float(np.quantile(p_test, 1.0 - cov_target))}
        else:
            taus = thresholds_per_class_for_target_coverage(p_eval, yhat_eval, K, cov_target)
            accept = decide_accept_per_class(p_test, yhat_test, taus)
            tau_dump = {int(k): float(v) for k, v in taus.items()}

        coverage, sel_acc, risk, per_class_cov, per_class_acc = selective_metrics(labels_np, yhat_np, accept, K)
        covs.append(coverage); risks.append(risk)

        row = {
            "target_coverage": float(cov_target),
            "achieved_coverage": float(coverage),
            "selective_accuracy_pct": float(sel_acc),
            "risk_pct": float(risk),
            "ECE_pct": float(ece),
            "baseline_accuracy_pct": float(baseline_acc),
            "calibrator_mode": calibrator_mode,
            "selection_mode": selection_mode,
            "thresholds": tau_dump,
        }
        # Add per-class columns
        for c in range(K):
            row[f"class_{c}_coverage"] = float(per_class_cov.get(c, 0.0))
            row[f"class_{c}_sel_acc_pct"] = float(per_class_acc.get(c, 0.0))
        rows.append(row)

    A = aurc(covs, risks)

    # Save CSV
    if csv_path is not None:
        df = pd.DataFrame(rows)
        df["AURC"] = A
        df.to_csv(csv_path, index=False)

    return {
        "per_coverage": rows,
        "AURC": A,
        "ECE": ece,
        "baseline_acc_pct": baseline_acc,
    }




def main():
    parser = argparse.ArgumentParser()

    ## Required parameters
    parser.add_argument('--project', type=str, required=True, help="using dataset from this project.")
    parser.add_argument("--train_data_file", default=None, type=str, required=True,
                        help="The input training data file (a text file).")
    parser.add_argument("--output_dir", default=None, type=str, required=True,
                        help="The output directory where the model predictions and checkpoints will be written.")

    ## Other parameters
    parser.add_argument("--eval_data_file", default=None, type=str,
                        help="An optional input evaluation data file to evaluate the perplexity on (a text file).")
    parser.add_argument("--test_data_file", default=None, type=str,
                        help="An optional input evaluation data file to evaluate the perplexity on (a text file).")
                    
    parser.add_argument("--model_type", default="bert", type=str,
                        help="The model architecture to be fine-tuned.")
    parser.add_argument("--model_name_or_path", default=None, type=str,
                        help="The model checkpoint for weights initialization.")

    parser.add_argument("--mlm", action='store_true',
                        help="Train with masked-language modeling loss instead of language modeling.")
    parser.add_argument("--mlm_probability", type=float, default=0.15,
                        help="Ratio of tokens to mask for masked language modeling loss")

    parser.add_argument("--config_name", default="", type=str,
                        help="Optional pretrained config name or path if not the same as model_name_or_path")
    parser.add_argument("--tokenizer_name", default="", type=str,
                        help="Optional pretrained tokenizer name or path if not the same as model_name_or_path")
    parser.add_argument("--cache_dir", default="", type=str,
                        help="Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)")
    parser.add_argument("--block_size", default=-1, type=int,
                        help="Optional input sequence length after tokenization."
                             "The training dataset will be truncated in block of this size for training."
                             "Default to the model max input length for single sentence inputs (take into account special tokens).")
    parser.add_argument("--do_train", action='store_true',
                        help="Whether to run training.")
    parser.add_argument("--do_eval", action='store_true',
                        help="Whether to run eval on the dev set.")
    parser.add_argument("--do_test", action='store_true',
                        help="Whether to run eval on the dev set.")    
    parser.add_argument("--evaluate_during_training", action='store_true',
                        help="Run evaluation during training at each logging step.")
    parser.add_argument("--do_lower_case", action='store_true',
                        help="Set this flag if you are using an uncased model.")

    parser.add_argument("--train_batch_size", default=4, type=int,
                        help="Batch size per GPU/CPU for training.")
    parser.add_argument("--eval_batch_size", default=4, type=int,
                        help="Batch size per GPU/CPU for evaluation.")
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,
                        help="Number of updates steps to accumulate before performing a backward/update pass.")
    parser.add_argument("--learning_rate", default=5e-5, type=float,
                        help="The initial learning rate for Adam.")
    parser.add_argument("--weight_decay", default=0.0, type=float,
                        help="Weight deay if we apply some.")
    parser.add_argument("--adam_epsilon", default=1e-8, type=float,
                        help="Epsilon for Adam optimizer.")
    parser.add_argument("--max_grad_norm", default=1.0, type=float,
                        help="Max gradient norm.")
    parser.add_argument("--num_train_epochs", default=2.0, type=float,
                        help="Total number of training epochs to perform.")
    parser.add_argument("--max_steps", default=-1, type=int,
                        help="If > 0: set total number of training steps to perform. Override num_train_epochs.")
    parser.add_argument("--warmup_steps", default=0, type=int,
                        help="Linear warmup over warmup_steps.")

    parser.add_argument('--logging_steps', type=int, default=50,
                        help="Log every X updates steps.")
    parser.add_argument('--save_steps', type=int, default=50,
                        help="Save checkpoint every X updates steps.")
    parser.add_argument('--save_total_limit', type=int, default=None,
                        help='Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default')
    parser.add_argument("--eval_all_checkpoints", action='store_true',
                        help="Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number")
    parser.add_argument("--no_cuda", action='store_true',
                        help="Avoid using CUDA when available")
    parser.add_argument('--overwrite_output_dir', action='store_true',
                        help="Overwrite the content of the output directory")
    parser.add_argument('--overwrite_cache', action='store_true',
                        help="Overwrite the cached training and evaluation sets")
    parser.add_argument('--seed', type=int, default=42,
                        help="random seed for initialization")
    parser.add_argument('--epoch', type=int, default=42,
                        help="random seed for initialization")
    parser.add_argument('--fp16', action='store_true',
                        help="Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit")
    parser.add_argument('--fp16_opt_level', type=str, default='O1',
                        help="For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
                             "See details at https://nvidia.github.io/apex/amp.html")
    parser.add_argument("--local_rank", type=int, default=-1,
                        help="For distributed training: local_rank")
    parser.add_argument('--server_ip', type=str, default='', help="For distant debugging.")
    parser.add_argument('--server_port', type=str, default='', help="For distant debugging.")

    # Add early stopping parameters and dropout probability parameters
    parser.add_argument("--early_stopping_patience", type=int, default=None,
                        help="Number of epochs with no improvement after which training will be stopped.")
    parser.add_argument("--min_loss_delta", type=float, default=0.001,
                        help="Minimum change in the loss required to qualify as an improvement.")
    parser.add_argument('--dropout_probability', type=float, default=0, help='dropout probability')


    

    args = parser.parse_args()

    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps) 
    device = accelerator.device
    args.n_gpu = accelerator.num_processes
    args.device = device
    args.per_gpu_train_batch_size=args.train_batch_size 
    args.per_gpu_eval_batch_size=args.eval_batch_size // args.n_gpu
    # Setup logging
    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                        datefmt='%m/%d/%Y %H:%M:%S',
                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)
    logger.warning("device: %s, n_gpu: %s, distributed training: %s",
                   device, args.n_gpu, bool(args.n_gpu > 1))
    #logger.info(accelerator.state, main_process_only=False)

    # Set seed
    set_seed(args.seed)

    args.start_epoch = 0
    args.start_step = 0
    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')
    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):
        args.model_name_or_path = os.path.join(checkpoint_last, 'model.bin')
        args.config_name = os.path.join(checkpoint_last, 'config.json')
        idx_file = os.path.join(checkpoint_last, 'idx_file.txt')
        with open(idx_file, encoding='utf-8') as idxf:
            args.start_epoch = int(idxf.readlines()[0].strip()) + 1

        step_file = os.path.join(checkpoint_last, 'step_file.txt')
        if os.path.exists(step_file):
            with open(step_file, encoding='utf-8') as stepf:
                args.start_step = int(stepf.readlines()[0].strip())

        logger.info("reload model from {}, resume from {} epoch".format(checkpoint_last, args.start_epoch))

    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]
    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,
                                          cache_dir=args.cache_dir if args.cache_dir else None)
    
    config.num_labels = 2
    if args.model_type not in ["codellama"]:
        tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,
                                                    do_lower_case=args.do_lower_case,
                                                    cache_dir=args.cache_dir if args.cache_dir else None)
    else:
        tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,
                                                    trust_remote_code=True)
    if args.block_size <= 0:
        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model
    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)

    compute_dtype = getattr(torch, "bfloat16")
    quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=False)

    if args.model_name_or_path:
        if args.model_type in ["starcoder3b", "deepseek","incoder1b"]:
            model = model_class.from_pretrained(args.model_name_or_path,
                                                quantization_config=quant_config, trust_remote_code=True,
                                                torch_dtype = torch.bfloat16)
                                                #attn_implementation = "flash_attention_2")
        else:
            print("load from here")
            model = model_class.from_pretrained(args.model_name_or_path,
                                                torch_dtype = torch.bfloat16, quantization_config=quant_config, trust_remote_code=True)
    else:
        model = model_class(config)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    #config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
    config.pad_token_id = tokenizer.pad_token_id

    #config.pad_token_id = tokenizer(tokenizer.pad_token, truncation=True)['input_ids'][0]
    if args.model_type in ['codellama', 'starcoder3b', "deepseek","incoder1b",'qwen7b']:
        model = DecoderClassifier(model,config,tokenizer,args)
    else:
        model = Model(model,config,tokenizer,args)

    logger.info("Training/evaluation parameters %s", args)
    
    if args.model_type in ['starcoder3b', 'incoder1b']:
        peft_params = LoraConfig(
        lora_alpha=16,
        lora_dropout=0.1,
        r=64,
        bias="none",
        task_type=TaskType.SEQ_CLS,   # sequence classification
        target_modules=["c_attn", "c_proj", "c_fc"]  # StarCoder modules
    )
    elif args.model_type in ['incoder1b']:
        peft_params = LoraConfig(
        task_type=TaskType.SEQ_CLS,   # decoder-only causal LM
        r=64,
        lora_alpha=16,
        lora_dropout=0.1,
        bias="none",
        # GPT-2/Incoder module names:
        target_modules=["c_attn", "c_proj", "c_fc"],
        fan_in_fan_out=True,                # important for GPT-2/Conv1D
        # keep classifier head (if any) trainable:
        
)
    else:
        peft_params = LoraConfig(
        lora_alpha=16,
        lora_dropout=0.1,
        r=64,
        bias="none",
        task_type=TaskType.SEQ_CLS
    )

    model = get_peft_model(model, peft_params)
    


    logger.info("Training/evaluation parameters %s", args)




    checkpoint_prefix = f'checkpoint-best-acc/{args.project}/model.bin'
    output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))  
    model.load_state_dict(torch.load(output_dir), strict=False)                  
    model.to(args.device)


    eval_dataset = TextDataset(tokenizer, args, args.eval_data_file)
    eval_sampler = SequentialSampler(eval_dataset)
    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,
                                 batch_size=args.eval_batch_size, num_workers=8, pin_memory=True)


    test_dataset = TextDataset(tokenizer, args, args.test_data_file)
    test_sampler = SequentialSampler(test_dataset)
    test_dataloader = DataLoader(test_dataset, sampler=test_sampler,
                                 batch_size=args.eval_batch_size, num_workers=8, pin_memory=True)

    #torch.Size([2732])
    #torch.Size([2732])
    #torch.Size([2732, 2])

    eval_probs, eval_label, eval_preds, eval_logits = get_predictions(model, eval_dataloader, args)
    eval_correctness = (eval_preds == eval_label).float()  # 1 for correct, 0 for incorrect
    print(eval_correctness)
    eval_erroneousness = 1 - eval_correctness  # Complement of correctness

    test_probs, test_label, test_preds, test_logits = get_predictions(model, test_dataloader, args)
    test_correctness = (test_preds == test_label).float()  # 1 for correct, 0 for incorrect
    test_erroneousness = 1 - test_correctness  # Complement of correctness

    K = int(getattr(args, "num_labels", 2))
    # 2) Run behavioral calibration
    out = behavioral_calibration_pipeline(
    eval_logits=eval_logits, eval_preds=eval_preds, eval_labels=eval_label, eval_probs=eval_probs,
     test_logits=test_logits, test_preds=test_preds, test_labels=test_label, test_probs=test_probs,
     num_classes=K,
     coverages=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],
     min_samples=25,
     calibrator_mode="global_logistic",     # try: "global_isotonic", "perclass_logistic", "perclass_isotonic"
     selection_mode="oracle_exact",         # exact coverage on TEST (for curves); use "calibrated" for deploy-like
     csv_path=f"{args.output_dir}/behavioral_calibration_results.csv",
     use_logits_margin=True,
     include_sr_feature=True
 )

    
    print("Baseline accuracy (no abstention): {:.2f}%".format(out["baseline_acc_pct"]))
    print("ECE (correctness calibration): {:.2f}%".format(out["ECE"]))
    print("AURC (risk–coverage area): {:.4f}".format(out["AURC"]))
    for r in out["per_coverage"]:
        print(f"[target {r['target_coverage']:.2f}] achieved={r['achieved_coverage']:.3f} "
           f"sel-acc={r['selective_accuracy_pct']:.2f}% risk={r['risk_pct']:.2f}%")
    print("  thresholds:", r["thresholds"])
    
if __name__ == '__main__':
    main()


